# This code imposed by the task of making Thread\ProcessPoolExecutor
# which will solve no more than N tasks. It should put solved tasks
# in some Queue. if Queue is full, then stop processing tasks.
# But it also shouldn't block client code (i.e. executor.submit)
#
# For example, the problem of generating N batches in advance to be
# supplied in fit_generator. You have infinite batches that can be
# generated by Augmentation, but you don't need more than N.
# Even though submitted tasks will be handled by K threads, they
# are submitted and it is a problem. We want to consume not more
# than N tasks at time.
#
# Related code also can be found in keras\utils\data_utils:
# SequenceEnqueuer, OrderEnqueuer and GenerateEnqueuer.
from time import sleep
import numpy as np
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor
import multiprocessing as mp
from functools import partial


def func_to_process(work_num):
    print('Work #{} started'.format(work_num))
    sleep(work_num / 10)
    return work_num


# unfortunately it should be global function
def __blocking_task(queue, func, arg):
    res = func(arg)
    queue.put(res)
    return res


def cool_map(pool, max_size, func, params):
    # always ready maxsize + n_workers tasks
    results = mp.Manager().Queue(maxsize=max_size)
    cool_task = partial(__blocking_task, results, func)

    pool.map(cool_task, params)
    return results


if __name__ == '__main__':
    params = np.random.randint(100, size=100)

    with ProcessPoolExecutor(6) as executor:
        queue = cool_map(executor, 1, func_to_process, params)

        for i in range(len(params)):
            print('Ready ready ready ready', queue.get())
