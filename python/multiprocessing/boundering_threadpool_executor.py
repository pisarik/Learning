# This code imposed by the task of making Thread\ProcessPoolExecutor
# which will solve no more than N tasks. It should put solved tasks
# in some Queue. if Queue is full, then stop processing tasks.
# But it also shouldn't block client code (i.e. executor.submit)
#
# For example, the problem of generating N batches in advance to be
# supplied in fit_generator. You have infinite batches that can be
# generated by Augmentation, but you don't need more than N.
# Even though submitted tasks will be handled by K threads, they
# are submitted and it is a problem. We want to consume not more
# than N tasks at time.
#
# Related code also can be found in keras\utils\data_utils:
# SequenceEnqueuer, OrderEnqueuer and GenerateEnqueuer.
from time import sleep
import numpy as np
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor
import multiprocessing as mp
from functools import partial


def func_to_process(work_num):
    print('Work #{} started'.format(work_num))
    sleep(work_num / 10)
    return work_num


def __blocking_task(queue, func, arg):
    res = func(arg)
    queue.put(res)
    return res


if __name__ == '__main__':
    # wrapper = lambda x: results_queue.put(func_to_process(x))
    params = np.random.randint(100, size=100)

    manager = mp.Manager()
    results = manager.Queue(maxsize=1)  # always ready maxsize + n_workers tasks

    with ProcessPoolExecutor(6) as executor:
        fs = executor.map(partial(__blocking_task, results, func_to_process),
                          params)

        for i in range(len(params)):
            print('Ready ready ready ready', results.get())
